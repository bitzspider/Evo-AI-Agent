# ğŸ¤– EVO AI Agent

An AI agent application with MCP (Model Context Protocol) integration, Gradio interface, Ollama, and extensible tool system.

## ğŸš€ Quick Start

### Prerequisites
- **Python 3.8+**
- **Ollama** installed and running
- **Node.js** (for some MCP servers)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/bitzspider/Evo-AI-Agent.git
   cd Evo-AI-Agent
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure environment**
   ```bash
   cp env.example .env
   # Edit .env with your settings
   ```

4. **Start Ollama** (if not already running)
   ```bash
   ollama serve
   ```

5. **Run the application**
   ```bash
   python app.py
   ```

6. **Access the interface**
   - Open your browser to `http://localhost:7860`
   - Start chatting with your AI agent!

## ğŸ“ Project Structure

```
Evo-AI-Agent/
â”œâ”€â”€ app.py                 # Main application
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ mcp_servers.json      # Tool configuration
â”œâ”€â”€ env.example           # Environment template
â”œâ”€â”€ custom_tools/         # Custom MCP tools
â”‚   â”œâ”€â”€ google_gmail.py   # Gmail integration
â”‚   â””â”€â”€ time_server.py    # Time/date operations
â””â”€â”€ README.md            # This file
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

## ğŸ”— Links

- **Repository**: [https://github.com/bitzspider/Evo-AI-Agent](https://github.com/bitzspider/Evo-AI-Agent)
- **Ollama**: [https://ollama.ai](https://ollama.ai)
- **MCP Specification**: [https://modelcontextprotocol.io](https://modelcontextprotocol.io)
- **Gradio**: [https://gradio.app](https://gradio.app)

## ğŸ™ Acknowledgments

- Built with [Gradio](https://gradio.app) for the web interface
- Powered by [Ollama](https://ollama.ai) for local LLM inference
- Implements [Model Context Protocol](https://modelcontextprotocol.io) for tool integration

---

**Made with â¤ï¸ for the AI community** 
